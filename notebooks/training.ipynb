{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training\n",
    "(WiP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import graph_class as gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "import torch\n",
    "from torch import nn\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset = gc.WeatherDataset('test_one')\n",
    "dataset.create('../data/data_initial_preprocessing.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "g = dataset.graph\n",
    "g = dgl.add_self_loop(g)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Graph(num_nodes=667, num_edges=3540,\n      ndata_schemes={'x': Scheme(shape=(1393, 7), dtype=torch.float64), 'y': Scheme(shape=(1393,), dtype=torch.float64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n      edata_schemes={})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Classe da Rede Neural\n",
    "class CGN(nn.Module):\n",
    "    def __init__(self, in_feats, num_classes):\n",
    "        super(CGN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, 32, norm='both')\n",
    "        self.conv2 = GraphConv(32, 16, norm='both')\n",
    "        self.conv3 = GraphConv(16, num_classes, norm='both')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = torch.tanh(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.tanh(h)\n",
    "        h = self.conv3(g, h)\n",
    "\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "net = CGN(g.ndata['x'].shape[2], 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "CGN(\n  (conv1): GraphConv(in=7, out=32, normalization=both, activation=None)\n  (conv2): GraphConv(in=32, out=16, normalization=both, activation=None)\n  (conv3): GraphConv(in=16, out=1, normalization=both, activation=None)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = net.float()\n",
    "net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "p = {\n",
    "    'epochs': 10000,\n",
    "    'optim': optim.Adam,\n",
    "    'loss_function': nn.MSELoss(),\n",
    "    'lr': 1e-3\n",
    "}\n",
    "\n",
    "net = net.to('cuda')\n",
    "g = g.to('cuda')\n",
    "name = 'runs/test_3'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.0887255668640137 Val Loss: 1.0006099939346313\n",
      "Epoch: 100 Loss: 0.945045530796051 Val Loss: 0.8729445934295654\n",
      "Epoch: 200 Loss: 0.9444595575332642 Val Loss: 0.8720039129257202\n",
      "Epoch: 300 Loss: 0.9440282583236694 Val Loss: 0.87148517370224\n",
      "Epoch: 400 Loss: 0.9435788989067078 Val Loss: 0.871043860912323\n",
      "Epoch: 500 Loss: 0.943105161190033 Val Loss: 0.8708491325378418\n",
      "Epoch: 600 Loss: 0.9426414370536804 Val Loss: 0.8709275722503662\n",
      "Epoch: 700 Loss: 0.9421799182891846 Val Loss: 0.8710023164749146\n",
      "Epoch: 800 Loss: 0.9416746497154236 Val Loss: 0.8708578944206238\n",
      "Epoch: 900 Loss: 0.9410627484321594 Val Loss: 0.870395839214325\n",
      "Epoch: 1000 Loss: 0.9402862787246704 Val Loss: 0.8695291876792908\n",
      "Epoch: 1100 Loss: 0.939276933670044 Val Loss: 0.8682030439376831\n",
      "Epoch: 1200 Loss: 0.9379695653915405 Val Loss: 0.8665600419044495\n",
      "Epoch: 1300 Loss: 0.9363989233970642 Val Loss: 0.8647010922431946\n",
      "Epoch: 1400 Loss: 0.9347147941589355 Val Loss: 0.8626964688301086\n",
      "Epoch: 1500 Loss: 0.9330533146858215 Val Loss: 0.8613716959953308\n",
      "Epoch: 1600 Loss: 0.9313899278640747 Val Loss: 0.8607481122016907\n",
      "Epoch: 1700 Loss: 0.9294973611831665 Val Loss: 0.8606017231941223\n",
      "Epoch: 1800 Loss: 0.9271467328071594 Val Loss: 0.8587828874588013\n",
      "Epoch: 1900 Loss: 0.9245402216911316 Val Loss: 0.8578002452850342\n",
      "Epoch: 2000 Loss: 0.9220902919769287 Val Loss: 0.8576902151107788\n",
      "Epoch: 2100 Loss: 0.9197930693626404 Val Loss: 0.8584179878234863\n",
      "Epoch: 2200 Loss: 0.9173684120178223 Val Loss: 0.8595554232597351\n",
      "Epoch: 2300 Loss: 0.9152562022209167 Val Loss: 0.8601669669151306\n",
      "Epoch: 2400 Loss: 0.9133996963500977 Val Loss: 0.8615477681159973\n",
      "Epoch: 2500 Loss: 0.9116315245628357 Val Loss: 0.863132655620575\n",
      "Epoch: 2600 Loss: 0.9099035263061523 Val Loss: 0.8646107912063599\n",
      "Epoch: 2700 Loss: 0.9083071351051331 Val Loss: 0.8665765523910522\n",
      "Epoch: 2800 Loss: 0.9064988493919373 Val Loss: 0.8664242029190063\n",
      "Epoch: 2900 Loss: 0.90482097864151 Val Loss: 0.8670237064361572\n",
      "Epoch: 3000 Loss: 0.9031895995140076 Val Loss: 0.8681401014328003\n",
      "Epoch: 3100 Loss: 0.9014345407485962 Val Loss: 0.8690637946128845\n",
      "Epoch: 3200 Loss: 0.899709165096283 Val Loss: 0.8698322772979736\n",
      "Epoch: 3300 Loss: 0.897873044013977 Val Loss: 0.8710036277770996\n",
      "Epoch: 3400 Loss: 0.8961719870567322 Val Loss: 0.872311532497406\n",
      "Epoch: 3500 Loss: 0.8943452835083008 Val Loss: 0.8740044236183167\n",
      "Epoch: 3600 Loss: 0.8924931883811951 Val Loss: 0.8761334419250488\n",
      "Epoch: 3700 Loss: 0.8902698755264282 Val Loss: 0.878825306892395\n",
      "Epoch: 3800 Loss: 0.8880174160003662 Val Loss: 0.8819327354431152\n",
      "Epoch: 3900 Loss: 0.8858325481414795 Val Loss: 0.8852360844612122\n",
      "Epoch: 4000 Loss: 0.8838824033737183 Val Loss: 0.8883639574050903\n",
      "Epoch: 4100 Loss: 0.881979763507843 Val Loss: 0.8916492462158203\n",
      "Epoch: 4200 Loss: 0.8802357912063599 Val Loss: 0.8949530124664307\n",
      "Epoch: 4300 Loss: 0.8785355091094971 Val Loss: 0.8983849883079529\n",
      "Epoch: 4400 Loss: 0.8769604563713074 Val Loss: 0.9017382860183716\n",
      "Epoch: 4500 Loss: 0.8754589557647705 Val Loss: 0.905117392539978\n",
      "Epoch: 4600 Loss: 0.8739955425262451 Val Loss: 0.9085156321525574\n",
      "Epoch: 4700 Loss: 0.8726497888565063 Val Loss: 0.9118730425834656\n",
      "Epoch: 4800 Loss: 0.8713802695274353 Val Loss: 0.9146483540534973\n",
      "Epoch: 4900 Loss: 0.870110034942627 Val Loss: 0.9183692336082458\n",
      "Epoch: 5000 Loss: 0.8689026236534119 Val Loss: 0.9215143918991089\n",
      "Epoch: 5100 Loss: 0.8678147196769714 Val Loss: 0.924451470375061\n",
      "Epoch: 5200 Loss: 0.8667372465133667 Val Loss: 0.9273393750190735\n",
      "Epoch: 5300 Loss: 0.8657041788101196 Val Loss: 0.9303800463676453\n",
      "Epoch: 5400 Loss: 0.8647167682647705 Val Loss: 0.9327550530433655\n",
      "Epoch: 5500 Loss: 0.8639047741889954 Val Loss: 0.9360488653182983\n",
      "Epoch: 5600 Loss: 0.8628156781196594 Val Loss: 0.9378373026847839\n",
      "Epoch: 5700 Loss: 0.8620378971099854 Val Loss: 0.9401169419288635\n",
      "Epoch: 5800 Loss: 0.8610140681266785 Val Loss: 0.9426631331443787\n",
      "Epoch: 5900 Loss: 0.8601388335227966 Val Loss: 0.945021390914917\n",
      "Epoch: 6000 Loss: 0.8593237996101379 Val Loss: 0.9472463726997375\n",
      "Epoch: 6100 Loss: 0.8585113883018494 Val Loss: 0.9495140314102173\n",
      "Epoch: 6200 Loss: 0.8577253818511963 Val Loss: 0.9517596960067749\n",
      "Epoch: 6300 Loss: 0.8569531440734863 Val Loss: 0.9538712501525879\n",
      "Epoch: 6400 Loss: 0.8561916947364807 Val Loss: 0.9558805227279663\n",
      "Epoch: 6500 Loss: 0.8554748296737671 Val Loss: 0.9579851031303406\n",
      "Epoch: 6600 Loss: 0.8547306656837463 Val Loss: 0.9599820375442505\n",
      "Epoch: 6700 Loss: 0.8540298938751221 Val Loss: 0.961818516254425\n",
      "Epoch: 6800 Loss: 0.8533473610877991 Val Loss: 0.9636213779449463\n",
      "Epoch: 6900 Loss: 0.8526472449302673 Val Loss: 0.965584933757782\n",
      "Epoch: 7000 Loss: 0.8519453406333923 Val Loss: 0.9669385552406311\n",
      "Epoch: 7100 Loss: 0.8512645363807678 Val Loss: 0.9685519933700562\n",
      "Epoch: 7200 Loss: 0.8505977392196655 Val Loss: 0.9700426459312439\n",
      "Epoch: 7300 Loss: 0.8499503135681152 Val Loss: 0.9711776971817017\n",
      "Epoch: 7400 Loss: 0.8493030071258545 Val Loss: 0.9727673530578613\n",
      "Epoch: 7500 Loss: 0.848680317401886 Val Loss: 0.9740167856216431\n",
      "Epoch: 7600 Loss: 0.8480587005615234 Val Loss: 0.9752553105354309\n",
      "Epoch: 7700 Loss: 0.847469687461853 Val Loss: 0.9763488173484802\n",
      "Epoch: 7800 Loss: 0.8468770384788513 Val Loss: 0.9775416254997253\n",
      "Epoch: 7900 Loss: 0.8463038206100464 Val Loss: 0.9787562489509583\n",
      "Epoch: 8000 Loss: 0.8457375168800354 Val Loss: 0.9797312617301941\n",
      "Epoch: 8100 Loss: 0.8460870981216431 Val Loss: 0.9833375215530396\n",
      "Epoch: 8200 Loss: 0.8446105718612671 Val Loss: 0.9819543957710266\n",
      "Epoch: 8300 Loss: 0.8440483212471008 Val Loss: 0.9831063151359558\n",
      "Epoch: 8400 Loss: 0.8434975147247314 Val Loss: 0.9843220710754395\n",
      "Epoch: 8500 Loss: 0.8429492115974426 Val Loss: 0.9853915572166443\n",
      "Epoch: 8600 Loss: 0.8424257636070251 Val Loss: 0.9866321086883545\n",
      "Epoch: 8700 Loss: 0.8418840765953064 Val Loss: 0.9878690242767334\n",
      "Epoch: 8800 Loss: 0.8413888812065125 Val Loss: 0.9889346957206726\n",
      "Epoch: 8900 Loss: 0.8408923149108887 Val Loss: 0.990138828754425\n",
      "Epoch: 9000 Loss: 0.8403835892677307 Val Loss: 0.9911258220672607\n",
      "Epoch: 9100 Loss: 0.8398800492286682 Val Loss: 0.9925536513328552\n",
      "Epoch: 9200 Loss: 0.8393781781196594 Val Loss: 0.9937506318092346\n",
      "Epoch: 9300 Loss: 0.8388983011245728 Val Loss: 0.9948288798332214\n",
      "Epoch: 9400 Loss: 0.8384671807289124 Val Loss: 0.9955776929855347\n",
      "Epoch: 9500 Loss: 0.8379442691802979 Val Loss: 0.9971001148223877\n",
      "Epoch: 9600 Loss: 0.8374959230422974 Val Loss: 0.9987790584564209\n",
      "Epoch: 9700 Loss: 0.8370152711868286 Val Loss: 0.999334454536438\n",
      "Epoch: 9800 Loss: 0.8365585207939148 Val Loss: 1.0004775524139404\n",
      "Epoch: 9900 Loss: 0.8361184597015381 Val Loss: 1.0014469623565674\n",
      "Epoch: 9999 Loss: 0.835684597492218 Val Loss: 1.0025956630706787\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(f\"{name}\")\n",
    "\n",
    "features = g.ndata['x'].float()\n",
    "label = g.ndata['y'].float()\n",
    "\n",
    "train_mask = g.ndata['train_mask']\n",
    "val_mask = g.ndata['val_mask']\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=p['lr'])\n",
    "\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "for epoch in range(p['epochs']):\n",
    "    prediction = net(g, features)\n",
    "    prediction = prediction.reshape(prediction.shape[0], prediction.shape[1])\n",
    "\n",
    "    loss = loss_fn(prediction[train_mask], label[train_mask])\n",
    "    val_loss = loss_fn(prediction[val_mask], label[val_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    history['loss'].append(loss.cpu().detach().numpy())\n",
    "    history['val_loss'].append(val_loss.cpu().detach().numpy())\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", loss, epoch) #tensorboard\n",
    "    writer.add_scalar(\"Loss/Val\", val_loss, epoch) #tensorboard\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch} Loss: {loss} Val Loss: {val_loss}')\n",
    "print(f'Epoch: {epoch} Loss: {loss} Val Loss: {val_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([533, 1400])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[train_mask].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([533, 1400])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[train_mask].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[11.39190968,  7.16351918,  5.75264139, ..., 15.19342615,\n        15.11701595, 15.01706286]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.scaler_y.inverse_transform(np.repeat(np.array(1.069), 1400).reshape(-1, 1400))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}