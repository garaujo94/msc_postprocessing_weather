{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training\n",
    "(WiP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import graph_class as gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "import torch\n",
    "from torch import nn\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dataset = gc.WeatherDataset('test_one')\n",
    "dataset.create('../data/data_initial_preprocessing.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "g = dataset.graph\n",
    "g = dgl.add_self_loop(g)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Graph(num_nodes=667, num_edges=4002,\n      ndata_schemes={'x': Scheme(shape=(1400, 8), dtype=torch.float64), 'y': Scheme(shape=(1400,), dtype=torch.float64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n      edata_schemes={})"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#Classe da Rede Neural\n",
    "class CGN(nn.Module):\n",
    "    def __init__(self, in_feats, num_classes):\n",
    "        super(CGN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, 32, norm='both')\n",
    "        self.conv2 = GraphConv(32, 16, norm='both')\n",
    "        self.conv3 = GraphConv(16, num_classes, norm='both')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = torch.tanh(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.tanh(h)\n",
    "        h = self.conv3(g, h)\n",
    "\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "net = CGN(g.ndata['x'].shape[2], 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "CGN(\n  (conv1): GraphConv(in=8, out=32, normalization=both, activation=None)\n  (conv2): GraphConv(in=32, out=16, normalization=both, activation=None)\n  (conv3): GraphConv(in=16, out=1, normalization=both, activation=None)\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = net.float()\n",
    "net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "p = {\n",
    "    'epochs': 10000,\n",
    "    'optim': optim.Adam,\n",
    "    'loss_function': nn.MSELoss(),\n",
    "    'lr': 1e-3\n",
    "}\n",
    "\n",
    "net = net.to('cuda')\n",
    "g = g.to('cuda')\n",
    "name = 'runs/test_2'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.6060307025909424 Val Loss: 1.4931100606918335\n",
      "Epoch: 100 Loss: 0.950023889541626 Val Loss: 0.8784621357917786\n",
      "Epoch: 200 Loss: 0.9491601586341858 Val Loss: 0.8780856728553772\n",
      "Epoch: 300 Loss: 0.9487327337265015 Val Loss: 0.8777257204055786\n",
      "Epoch: 400 Loss: 0.9484791159629822 Val Loss: 0.8774576783180237\n",
      "Epoch: 500 Loss: 0.9482951164245605 Val Loss: 0.8772980570793152\n",
      "Epoch: 600 Loss: 0.9481412172317505 Val Loss: 0.877231240272522\n",
      "Epoch: 700 Loss: 0.9480010271072388 Val Loss: 0.877230167388916\n",
      "Epoch: 800 Loss: 0.9478656649589539 Val Loss: 0.8772702217102051\n",
      "Epoch: 900 Loss: 0.9477286338806152 Val Loss: 0.8773313760757446\n",
      "Epoch: 1000 Loss: 0.9475834369659424 Val Loss: 0.8773981928825378\n",
      "Epoch: 1100 Loss: 0.9474226236343384 Val Loss: 0.877459704875946\n",
      "Epoch: 1200 Loss: 0.9472373127937317 Val Loss: 0.8775104284286499\n",
      "Epoch: 1300 Loss: 0.9470175504684448 Val Loss: 0.8775526285171509\n",
      "Epoch: 1400 Loss: 0.9467513561248779 Val Loss: 0.8776010870933533\n",
      "Epoch: 1500 Loss: 0.9464264512062073 Val Loss: 0.8776915669441223\n",
      "Epoch: 1600 Loss: 0.9460319876670837 Val Loss: 0.8778882026672363\n",
      "Epoch: 1700 Loss: 0.9455658793449402 Val Loss: 0.8782747983932495\n",
      "Epoch: 1800 Loss: 0.9450425505638123 Val Loss: 0.8789212703704834\n",
      "Epoch: 1900 Loss: 0.9444931149482727 Val Loss: 0.8798345923423767\n",
      "Epoch: 2000 Loss: 0.943952739238739 Val Loss: 0.880931556224823\n",
      "Epoch: 2100 Loss: 0.9434424042701721 Val Loss: 0.8820652961730957\n",
      "Epoch: 2200 Loss: 0.9429572820663452 Val Loss: 0.8830861449241638\n",
      "Epoch: 2300 Loss: 0.9424747228622437 Val Loss: 0.8838964104652405\n",
      "Epoch: 2400 Loss: 0.9419706463813782 Val Loss: 0.8844801187515259\n",
      "Epoch: 2500 Loss: 0.9414323568344116 Val Loss: 0.8849121332168579\n",
      "Epoch: 2600 Loss: 0.9408596754074097 Val Loss: 0.8853388428688049\n",
      "Epoch: 2700 Loss: 0.9402471780776978 Val Loss: 0.8858783841133118\n",
      "Epoch: 2800 Loss: 0.9395433664321899 Val Loss: 0.8867725133895874\n",
      "Epoch: 2900 Loss: 0.9387030005455017 Val Loss: 0.888274610042572\n",
      "Epoch: 3000 Loss: 0.9377073645591736 Val Loss: 0.8901115655899048\n",
      "Epoch: 3100 Loss: 0.9364668130874634 Val Loss: 0.8917884826660156\n",
      "Epoch: 3200 Loss: 0.9350166320800781 Val Loss: 0.8940349221229553\n",
      "Epoch: 3300 Loss: 0.9335275292396545 Val Loss: 0.8968390226364136\n",
      "Epoch: 3400 Loss: 0.9320934414863586 Val Loss: 0.8999519348144531\n",
      "Epoch: 3500 Loss: 0.9307114481925964 Val Loss: 0.9030919075012207\n",
      "Epoch: 3600 Loss: 0.9293131828308105 Val Loss: 0.905987024307251\n",
      "Epoch: 3700 Loss: 0.9278308153152466 Val Loss: 0.907792866230011\n",
      "Epoch: 3800 Loss: 0.9262147545814514 Val Loss: 0.9101448655128479\n",
      "Epoch: 3900 Loss: 0.9245116710662842 Val Loss: 0.9118704199790955\n",
      "Epoch: 4000 Loss: 0.922751247882843 Val Loss: 0.9135356545448303\n",
      "Epoch: 4100 Loss: 0.9209967851638794 Val Loss: 0.915332019329071\n",
      "Epoch: 4200 Loss: 0.9192838072776794 Val Loss: 0.9166697859764099\n",
      "Epoch: 4300 Loss: 0.9176437258720398 Val Loss: 0.9175617694854736\n",
      "Epoch: 4400 Loss: 0.9160937070846558 Val Loss: 0.9173274636268616\n",
      "Epoch: 4500 Loss: 0.9146798849105835 Val Loss: 0.9168855547904968\n",
      "Epoch: 4600 Loss: 0.9133800864219666 Val Loss: 0.9169602990150452\n",
      "Epoch: 4700 Loss: 0.9120857119560242 Val Loss: 0.9155926704406738\n",
      "Epoch: 4800 Loss: 0.9108502864837646 Val Loss: 0.914977490901947\n",
      "Epoch: 4900 Loss: 0.9096453785896301 Val Loss: 0.9142330884933472\n",
      "Epoch: 5000 Loss: 0.9084345698356628 Val Loss: 0.9134738445281982\n",
      "Epoch: 5100 Loss: 0.9072332978248596 Val Loss: 0.9128686785697937\n",
      "Epoch: 5200 Loss: 0.9060375690460205 Val Loss: 0.9120753407478333\n",
      "Epoch: 5300 Loss: 0.9048470258712769 Val Loss: 0.9113566875457764\n",
      "Epoch: 5400 Loss: 0.9036136269569397 Val Loss: 0.9113298058509827\n",
      "Epoch: 5500 Loss: 0.9023575186729431 Val Loss: 0.9107496738433838\n",
      "Epoch: 5600 Loss: 0.9010665416717529 Val Loss: 0.910440981388092\n",
      "Epoch: 5700 Loss: 0.8997414112091064 Val Loss: 0.9103314876556396\n",
      "Epoch: 5800 Loss: 0.8983692526817322 Val Loss: 0.9102498888969421\n",
      "Epoch: 5900 Loss: 0.8969110250473022 Val Loss: 0.9101176857948303\n",
      "Epoch: 6000 Loss: 0.8953646421432495 Val Loss: 0.9100285172462463\n",
      "Epoch: 6100 Loss: 0.8937159776687622 Val Loss: 0.90975022315979\n",
      "Epoch: 6200 Loss: 0.8920043706893921 Val Loss: 0.910433292388916\n",
      "Epoch: 6300 Loss: 0.8902642726898193 Val Loss: 0.9109195470809937\n",
      "Epoch: 6400 Loss: 0.8885352611541748 Val Loss: 0.9116883873939514\n",
      "Epoch: 6500 Loss: 0.8868602514266968 Val Loss: 0.9123011231422424\n",
      "Epoch: 6600 Loss: 0.8851587772369385 Val Loss: 0.9127755761146545\n",
      "Epoch: 6700 Loss: 0.8835593461990356 Val Loss: 0.913415253162384\n",
      "Epoch: 6800 Loss: 0.881959080696106 Val Loss: 0.9136044383049011\n",
      "Epoch: 6900 Loss: 0.8804018497467041 Val Loss: 0.9142312407493591\n",
      "Epoch: 7000 Loss: 0.878820538520813 Val Loss: 0.9152841567993164\n",
      "Epoch: 7100 Loss: 0.8772980570793152 Val Loss: 0.9160086512565613\n",
      "Epoch: 7200 Loss: 0.875846803188324 Val Loss: 0.9172656536102295\n",
      "Epoch: 7300 Loss: 0.8743942379951477 Val Loss: 0.917557954788208\n",
      "Epoch: 7400 Loss: 0.8730018734931946 Val Loss: 0.9186882376670837\n",
      "Epoch: 7500 Loss: 0.8717031478881836 Val Loss: 0.9197635650634766\n",
      "Epoch: 7600 Loss: 0.8704734444618225 Val Loss: 0.920304000377655\n",
      "Epoch: 7700 Loss: 0.8693164587020874 Val Loss: 0.9212902188301086\n",
      "Epoch: 7800 Loss: 0.8682476282119751 Val Loss: 0.9221711158752441\n",
      "Epoch: 7900 Loss: 0.8672544360160828 Val Loss: 0.9229393005371094\n",
      "Epoch: 8000 Loss: 0.8663315773010254 Val Loss: 0.9236180782318115\n",
      "Epoch: 8100 Loss: 0.8654744625091553 Val Loss: 0.9244255423545837\n",
      "Epoch: 8200 Loss: 0.8646450042724609 Val Loss: 0.9248508214950562\n",
      "Epoch: 8300 Loss: 0.8638733625411987 Val Loss: 0.9251747131347656\n",
      "Epoch: 8400 Loss: 0.8631147742271423 Val Loss: 0.9258435964584351\n",
      "Epoch: 8500 Loss: 0.8623802065849304 Val Loss: 0.9264785647392273\n",
      "Epoch: 8600 Loss: 0.8616840243339539 Val Loss: 0.9268625378608704\n",
      "Epoch: 8700 Loss: 0.8609978556632996 Val Loss: 0.9273593425750732\n",
      "Epoch: 8800 Loss: 0.8603307604789734 Val Loss: 0.9276073575019836\n",
      "Epoch: 8900 Loss: 0.8596760034561157 Val Loss: 0.9280088543891907\n",
      "Epoch: 9000 Loss: 0.8591270446777344 Val Loss: 0.9286466240882874\n",
      "Epoch: 9100 Loss: 0.858407735824585 Val Loss: 0.9286422729492188\n",
      "Epoch: 9200 Loss: 0.8577980995178223 Val Loss: 0.9290833473205566\n",
      "Epoch: 9300 Loss: 0.8572101593017578 Val Loss: 0.9292993545532227\n",
      "Epoch: 9400 Loss: 0.8566180467605591 Val Loss: 0.9296703934669495\n",
      "Epoch: 9500 Loss: 0.8560308814048767 Val Loss: 0.9300857186317444\n",
      "Epoch: 9600 Loss: 0.8560412526130676 Val Loss: 0.9306031465530396\n",
      "Epoch: 9700 Loss: 0.8549433946609497 Val Loss: 0.9308356046676636\n",
      "Epoch: 9800 Loss: 0.854415774345398 Val Loss: 0.9312112331390381\n",
      "Epoch: 9900 Loss: 0.8538992404937744 Val Loss: 0.9316427707672119\n",
      "Epoch: 9999 Loss: 0.8534048795700073 Val Loss: 0.9320451617240906\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(f\"{name}\")\n",
    "\n",
    "features = g.ndata['x'].float()\n",
    "label = g.ndata['y'].float()\n",
    "\n",
    "train_mask = g.ndata['train_mask']\n",
    "val_mask = g.ndata['val_mask']\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=p['lr'])\n",
    "\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "for epoch in range(p['epochs']):\n",
    "    prediction = net(g, features)\n",
    "    prediction = prediction.reshape(prediction.shape[0], prediction.shape[1])\n",
    "\n",
    "    loss = loss_fn(prediction[train_mask], label[train_mask])\n",
    "    val_loss = loss_fn(prediction[val_mask], label[val_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    history['loss'].append(loss.cpu().detach().numpy())\n",
    "    history['val_loss'].append(val_loss.cpu().detach().numpy())\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", loss, epoch) #tensorboard\n",
    "    writer.add_scalar(\"Loss/Val\", val_loss, epoch) #tensorboard\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch} Loss: {loss} Val Loss: {val_loss}')\n",
    "print(f'Epoch: {epoch} Loss: {loss} Val Loss: {val_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([533, 1400])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[train_mask].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([533, 1400])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[train_mask].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[11.39190968,  7.16351918,  5.75264139, ..., 15.19342615,\n        15.11701595, 15.01706286]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.scaler_y.inverse_transform(np.repeat(np.array(1.069), 1400).reshape(-1, 1400))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}